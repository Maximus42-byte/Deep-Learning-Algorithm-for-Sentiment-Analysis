# -*- coding: utf-8 -*-
"""RNN_model_#3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LvVEHdIS2TZhnOwthlvkscYVR2xvdFI3
"""

!pip install keras.preprocessing
!pip install keras.models
!pip install numpy
!pip install pandas
!pip install matplotlib

import pandas as pd
data = pd.read_csv('/content/drive/My Drive/train_comments.csv')
target = data['verification_status']
text = data['comment']
#import
import re
import numpy as np
from sklearn.model_selection import  train_test_split
import matplotlib.pyplot as plt
from keras.models import Sequential , load_model
from keras.layers import Dense, LSTM, Embedding, Dropout
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

data

target.value_counts().sort_index().plot.bar()

target1 = target
text1 = text
list_pos = []
list_neg = []

for i in range(0,180000):
  if target[i] == 1:
    list_neg.append(text[i])

len(list_neg)

z=0
for i in range(0,180000):
  if z == 30000:
    break
  if target[i] == 0:
    list_pos.append(text[i])
    z = z+1

len(list_pos)

#target = target.drop(z).reset_index(drop=True)
#text = text.drop(z).reset_index(drop=True)
negative_series = pd.Series(list_neg)
positive_series = pd.Series(list_pos)

result = negative_series.append(positive_series, ignore_index = True)

result

labels = []
for i in range(60000):
  if i<30000:
    labels.append(1)
  if i>29999:
    labels.append(0)

label_series = pd.Series(labels)

label_series

new_data  = {"verification_status": label_series,"comment": result}
df = pd.concat(new_data,axis = 1)

df = df.sample(frac=1).reset_index(drop=True)

df2 = df.head()

df.count()

new_text = df['comment']
new_target = df['verification_status']

new_target.value_counts().sort_index().plot.bar()

target2 = new_target[0:50000]
text2 = new_text[0:50000]

target3 = new_target[50001:]
text3 = new_text[50001:]

for i in range(50000):
  text2[i] = str(text2[i])  
for i in range(50001,60000):
  text3[i] = str(text3[i])

token = Tokenizer(num_words=20000)
token.fit_on_texts(text2.values)

X = token.texts_to_sequences(text2.values)
X = pad_sequences(X)

model = Sequential()
model.add(Embedding(20000, 256,))
model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1,activation='sigmoid'))

model.compile(loss='binary_crossentropy',optimizer='adam'
,metrics=['accuracy'])
model.summary()

from tensorflow.keras.preprocessing import sequence
#y= pd.get_dummies(target2).values
y = target2
X_train , x_test , Y_train , y_test = train_test_split(X, y, 
                                                       test_size = 0.2 ,
                                                       random_state = 0)
X_train = sequence.pad_sequences(X_train,maxlen = 100)
x_test = sequence.pad_sequences(x_test,maxlen = 100)

batch_size = 32
epochs = 10
model.fit(X_train , Y_train, epochs=epochs ,
          batch_size= batch_size ,verbose=2
          ,validation_data=(x_test,y_test))

x_test

score , acc = model.evaluate(x_test,y_test,batch_size=32,verbose=2)
print("Test score :  ",score)
print("Test accuracy :  " , acc)